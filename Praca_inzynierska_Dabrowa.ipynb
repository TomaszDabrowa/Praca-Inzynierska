{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-temporal analysis of vegetation indices variability\n",
    "Made by Tomasz DÄ…browa\n",
    "\n",
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import json, requests\n",
    "import datetime\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from shapely.geometry import mapping, shape, Polygon, MultiPolygon\n",
    "from shapely.wkt import dumps\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import statistics\n",
    "#from osgeo import gdal, osr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining useful functions\n",
    "\n",
    "Those functions are responsible for obtaining specified data from path and to make projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(link):\n",
    "    link_divided = link.split('/')\n",
    "    return link_divided[5]\n",
    "def getMonth(link):\n",
    "    link_divided = link.split('/')\n",
    "    return link_divided[6]\n",
    "def getDay(link):\n",
    "    link_divided = link.split('/')\n",
    "    return link_divided[7]\n",
    "def getDate(link):\n",
    "    link_divided = link.split('/')\n",
    "    return link_divided[5] + '-' + link_divided[6] + '-' + link_divided[7]\n",
    "def getTile(link):\n",
    "    link_divided = link.split('/')\n",
    "    last = link_divided[len(link_divided)-1]\n",
    "    return last[0:6]\n",
    "def getBand(link):\n",
    "    link_divided = link.split('/')\n",
    "    last = link_divided[len(link_divided)-1]\n",
    "    return last[23:26]\n",
    "# def transformToEPSG2180(path, id):\n",
    "#     src_ds = gdal.Open(path)\n",
    "#     src_wkt = src_ds.GetProjection()\n",
    "#     src_srs = osr.SpatialReference()\n",
    "#     src_srs.ImportFromWkt(src_wkt)\n",
    "#     dst_srs = osr.SpatialReference()\n",
    "#     dst_srs.ImportFromEPSG(2180)\n",
    "#     output_raster = f\"/tmp/{id}.tif\"\n",
    "#     gdal.Warp(output_raster, src_ds, dstSRS=dst_srs.ExportToWkt())\n",
    "\n",
    "def printReport(links_df, dates, values):\n",
    "    print('====================== PROCESSING REPORT =========================')\n",
    "    if len(links_df) == len(dates):\n",
    "        print(\"All sattelite images processed\")\n",
    "    elif len(dates) != len(values):\n",
    "        print(\"ERROR -> dates are not equal to indices values\")\n",
    "    else:\n",
    "        print(\"Not every sattelite image processed because of the cloud cover\")\n",
    "        print('Available: ', len(links_df),'Used: ',len(dates))\n",
    "    print('==================================================================')\n",
    "\n",
    "def getFileName(path_to_file):\n",
    "    full_path = path_to_file.split('/')\n",
    "    filename = full_path[len(full_path)-1]\n",
    "    filename = filename.split('.')\n",
    "    filename_without_extension = filename[0]\n",
    "    return filename_without_extension\n",
    "\n",
    "def coordinatesToRasterEPSG(crs, coordinates):\n",
    "    coordinates_in_raster_epsg = coordinates.to_crs(crs)\n",
    "    geometry_raster_EPSG = coordinates_in_raster_epsg['geometry'][0]\n",
    "    if isinstance(geometry_raster_EPSG, MultiPolygon):\n",
    "        location_shapely_raster_EPSG = geometry_raster_EPSG.geoms[0]\n",
    "    else:\n",
    "        location_shapely_raster_EPSG = geometry_raster_EPSG\n",
    "    return location_shapely_raster_EPSG\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying input parameters\n",
    "Choose time period that interests you for the analysis purposes.\n",
    "- Specify start and end date in format YYYY-MM-DD.\n",
    "- Type path to your ESRI Shapefile in EPSG 2180 containing one polygon or multipolygon\n",
    "\n",
    "DO NOT change resolution parameter unless you know what you are doing (may cause errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str(\"2023-01-01\")  \n",
    "end = str(\"2023-12-31\")\n",
    "shapefile_path = 'workspace/dummy_epsg2180.shp'\n",
    "\n",
    "resolution = '20m' # DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading vector data \n",
    "Vector data that you provided is now projected to EPSG 4326 in order to make it usable for url request.\n",
    "If data contains multipolygon type, only the first polygon is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = gpd.read_file(shapefile_path)\n",
    "coordinates_EPSG4326 = coordinates.to_crs('EPSG:4326')\n",
    "geometry_EPSG4326 = coordinates_EPSG4326['geometry'][0]\n",
    "if isinstance(geometry_EPSG4326, MultiPolygon):\n",
    "    # Choosing the first object from MultiPolygon object\n",
    "    location_shapely = geometry_EPSG4326.geoms[0]\n",
    "    print(location_shapely)\n",
    "else:\n",
    "    # if the geometry is the Polygon type already\n",
    "    location_shapely = geometry_EPSG4326\n",
    "location = str(location_shapely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending query by ODATA \n",
    "Sending request in order to get path for directories that contain sattelite date in eodata catalog.\n",
    "A response is sorted by data ascending in time.\n",
    "If we have more than one image per day, links to additional images are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://datahub.creodias.eu/odata/v1/Products?$filter=((Attributes/OData.CSC.DoubleAttribute/any(i0:i0/Name eq 'cloudCover' and i0/Value le 20)) and (ContentDate/Start ge {start}T00:00:00.000Z and ContentDate/Start le {end}T23:59:59.999Z) and (Online eq true) and (OData.CSC.Intersects(Footprint=geography'SRID=4326;{location}')) and (((((((Attributes/OData.CSC.StringAttribute/any(i0:i0/Name eq 'productType' and i0/Value eq 'S2MSI2A')))) and (Collection/Name eq 'SENTINEL-2'))))))&$expand=Attributes&$top=1000\"\n",
    "products = json.loads(requests.get(url).text)\n",
    "tmp_cloud_cover = []    \n",
    "path_list = []                           \n",
    "for item in products['value']:\n",
    "    path_list.append(item['S3Path'])     #append each S3Path (path) to the list\n",
    "    #print(item['S3Path'])        \n",
    "    for attribute in item['Attributes']:\n",
    "        if attribute['Name'] == 'cloudCover':\n",
    "            tmp_cloud_cover.append(attribute['Value'])\n",
    "            break\n",
    "\n",
    "path_list_sorted = sorted(path_list, key=getDate)\n",
    "\n",
    "path_list_sorted_cut = []\n",
    "seen_dates = set()\n",
    "for path in path_list_sorted:\n",
    "    date = getDate(path)\n",
    "    if date not in seen_dates:\n",
    "        path_list_sorted_cut.append(path)\n",
    "        seen_dates.add(date)\n",
    "path_list_sorted_cut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Table of Links\n",
    "A Pandas Dataframe is created that store information about sattelite data in format that is described below:\n",
    "- directory_path: absolute path to .SAFE directory\n",
    "- date: date of data\n",
    "- tile_id: id of tile\n",
    "- cloud_cover: parameter describing cloud coverage in %\n",
    "- RED: path to red band (B04)\n",
    "- NIR: path to near-infrared band (B8A)\n",
    "- SCL: path to scene classification layer\n",
    "- B05: path to B05 band\n",
    "- B11: path to B11 band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "i_for_cloud_cover = 0\n",
    "for directory_path in path_list_sorted_cut:\n",
    "    \n",
    "    matching_directories = glob.glob(os.path.join(directory_path, f\"GRANULE/*/IMG_DATA/R{resolution}\"))\n",
    "\n",
    "    for directory in matching_directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                tmp_band = getBand(os.path.join(root, file))\n",
    "                if tmp_band == 'B04':\n",
    "                    tmp_red_path = os.path.join(root, file)\n",
    "                if tmp_band == 'B8A':\n",
    "                    tmp_nir_path = os.path.join(root, file)\n",
    "                if tmp_band == 'SCL':\n",
    "                    tmp_scl_path = os.path.join(root, file)\n",
    "                if tmp_band == 'B05':\n",
    "                    tmp_b05_path = os.path.join(root, file)\n",
    "                if tmp_band == 'B11':\n",
    "                    tmp_b11_path = os.path.join(root, file)\n",
    "                \n",
    "            links.append([directory_path, \n",
    "                          getDate(os.path.join(root, file)), \n",
    "                          getTile(os.path.join(root, file)), \n",
    "                          tmp_cloud_cover[i_for_cloud_cover], \n",
    "                          tmp_red_path, \n",
    "                          tmp_nir_path, \n",
    "                          tmp_scl_path,\n",
    "                          tmp_b05_path,\n",
    "                          tmp_b11_path])\n",
    "            \n",
    "    i_for_cloud_cover = i_for_cloud_cover + 1\n",
    "\n",
    "    \"\"\"\n",
    "    links is an output. It is a list of lists. Each list has a structure that is described below:\n",
    "    [directory path (absolute path ending on .SAFE catalog), date of image, Tile ID, Cloud cover (in %), path to red band, path to nir band, path to SCL band, path to B05, path to B11 band]\n",
    "    \"\"\"\n",
    "links_df = pd.DataFrame(links, columns=['directory_path', 'date', 'tile_id', 'cloud_cover', 'RED', 'NIR', 'SCL', 'B05', 'B11'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of database with paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices computing\n",
    "That is the main part of code. Iterating by date, all required indices are computed. Firstly, scene classification layer is opened and projected to EPSG 2180. If the mode indicates clouds or snow, the next image starts to be processed. If not, other bands are projected and indices are computed.\n",
    "At the end, the report is printed.\n",
    "\n",
    "It may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "ndvi_means = []\n",
    "ndre_means = []\n",
    "savi_means = []\n",
    "ndii_means = []\n",
    "evi2_means = []\n",
    "\n",
    "for i in range(len(links_df)):\n",
    "\n",
    "    print(round(i/len(links)*100,0),'%','    Processing date: ', links_df['date'][i])\n",
    "\n",
    "    with rasterio.open(links_df['SCL'][i]) as scl_canal_src:\n",
    "        location_shapely_raster_EPSG = coordinatesToRasterEPSG(scl_canal_src.crs, coordinates)\n",
    "        scl_canal, scl_canal_transform = rasterio.mask.mask(scl_canal_src, [location_shapely_raster_EPSG], crop=True)\n",
    "        scl_canal_meta = scl_canal_src.meta\n",
    "        scl_canal_meta.update({\"driver\": \"GTiff\",\n",
    "                    \"height\": scl_canal.shape[1],\n",
    "                    \"width\": scl_canal.shape[2],\n",
    "                    \"transform\": scl_canal_transform})\n",
    "\n",
    "        scl_canal_flattened = scl_canal.flatten().astype(float)\n",
    "        scl_canal_flattened[scl_canal_flattened == 0] = np.nan\n",
    "        scl_mode = statistics.mode(scl_canal_flattened)\n",
    "\n",
    "        # checking if there is not a high cloud coverage or snow\n",
    "        if scl_mode not in [3, 8, 9, 11]:\n",
    "            \n",
    "            with rasterio.open(links_df['RED'][i]) as red_canal_src, rasterio.open(links_df['NIR'][i]) as nir_canal_src, rasterio.open(links_df['B05'][i]) as b05_canal_src, rasterio.open(links_df['B11'][i]) as b11_canal_src:\n",
    "                # other bands clipping\n",
    "                #RED\n",
    "                red_canal, red_canal_transform = rasterio.mask.mask(red_canal_src, [location_shapely_raster_EPSG], crop=True)\n",
    "                red_canal_meta = red_canal_src.meta\n",
    "                red_canal_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": red_canal.shape[1],\n",
    "                            \"width\": red_canal.shape[2],\n",
    "                            \"transform\": red_canal_transform})\n",
    "                \n",
    "                #NIR\n",
    "                nir_canal, nir_canal_transform = rasterio.mask.mask(nir_canal_src, [location_shapely_raster_EPSG], crop=True)\n",
    "                nir_canal_meta = nir_canal_src.meta\n",
    "                nir_canal_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": nir_canal.shape[1],\n",
    "                            \"width\": nir_canal.shape[2],\n",
    "                            \"transform\": nir_canal_transform})\n",
    "                \n",
    "\n",
    "                #B05\n",
    "                b05_canal, b05_canal_transform = rasterio.mask.mask(b05_canal_src, [location_shapely_raster_EPSG], crop=True)\n",
    "                b05_canal_meta = b05_canal_src.meta\n",
    "                b05_canal_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": b05_canal.shape[1],\n",
    "                            \"width\": b05_canal.shape[2],\n",
    "                            \"transform\": b05_canal_transform})\n",
    "                #B11\n",
    "                b11_canal, b11_canal_transform = rasterio.mask.mask(b11_canal_src, [location_shapely_raster_EPSG], crop=True)\n",
    "                b11_canal_meta = b11_canal_src.meta\n",
    "                b11_canal_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": b11_canal.shape[1],\n",
    "                            \"width\": b11_canal.shape[2],\n",
    "                            \"transform\": b11_canal_transform})\n",
    "                \n",
    "\n",
    "                nan_mask = np.isnan(nir_canal[0])\n",
    "                \n",
    "                # NDVI computing (and dividing by zero errors disabling)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    ndvi = (nir_canal[0] - red_canal[0]) / (nir_canal[0] + red_canal[0])\n",
    "\n",
    "                \n",
    "                # masking the water\n",
    "                ndvi[scl_canal[0] == 6] = np.nan\n",
    "\n",
    "                # switching to NaN NDVI values that are not in the range\n",
    "                ndvi[ndvi > 1] = np.nan\n",
    "\n",
    "                # NDRE computing (and dividing by zero errors disabling)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    ndre = (nir_canal[0] - b05_canal[0]) / (nir_canal[0] + b05_canal[0])\n",
    "\n",
    "                # water masking\n",
    "                ndre[scl_canal[0] == 6] = np.nan\n",
    "\n",
    "                # switching to NaN NDRE values that are not in the range\n",
    "                ndre[ndre > 1] = np.nan\n",
    "\n",
    "                # Computing SAVI\n",
    "                \n",
    "                #describing l parameter for SAVI index based on SCL band values\n",
    "\n",
    "                if scl_mode == 4:\n",
    "                    l = 0.1\n",
    "                else:\n",
    "                    l = 0.5\n",
    "\n",
    "                savi = ((nir_canal[0] - red_canal[0]) / (nir_canal[0] + red_canal[0] + l)) * (1 + l)\n",
    "\n",
    "                \n",
    "                savi[nan_mask] = np.nan\n",
    "\n",
    "                #water masking\n",
    "                savi[scl_canal[0] == 6] = np.nan\n",
    "\n",
    "                # switching to NaN SAVI values that are not in the range\n",
    "                savi[savi > 1] = np.nan\n",
    "                savi[savi == 0] = np.nan\n",
    "\n",
    "\n",
    "                # Computing NDII\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    ndii = (nir_canal[0] - b11_canal[0]) / (nir_canal[0] + b11_canal[0])\n",
    "\n",
    "                ndii[ndii > 1] = np.nan\n",
    "\n",
    "\n",
    "                # Computing EVI2\n",
    "                evi2 = 2.5 * ( (nir_canal[0] - red_canal[0]) / (nir_canal[0] + (2.4*red_canal[0]) + 1))\n",
    "\n",
    "                # NaN masking\n",
    "                evi2[evi2 == 0] = np.nan\n",
    "                #water masking\n",
    "                evi2[scl_canal[0] == 6] = np.nan\n",
    "                #masking values upper than 2\n",
    "                evi2[evi2 >= 2] = np.nan\n",
    "                \n",
    "\n",
    "                ndvi_mean = np.nanmean(ndvi)\n",
    "                ndvi_means.append(ndvi_mean)\n",
    "\n",
    "                ndre_mean = np.nanmean(ndre)\n",
    "                ndre_means.append(ndre_mean)\n",
    "\n",
    "                savi_mean = np.nanmean(savi)\n",
    "                savi_means.append(savi_mean)\n",
    "\n",
    "                ndii_mean = np.nanmean(ndii)\n",
    "                ndii_means.append(ndii_mean)\n",
    "\n",
    "                evi2_mean = np.nanmean(evi2)\n",
    "                evi2_means.append(evi2_mean)\n",
    "\n",
    "                dates.append(links_df['date'][i])\n",
    "                \n",
    "        else:\n",
    "            print(f\"Data from {links_df['date'][i]} not used\")\n",
    "\n",
    "print(100,'%')\n",
    "printReport(links_df, dates, ndvi_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results\n",
    "Results are ploted and saved to JPG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_export = getFileName(shapefile_path) + f\"_{start}_{end}\"\n",
    "dates_str = [datetime.datetime.strptime(date, \"%Y-%m-%d\") for date in dates]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_str, ndvi_means, 'go', label='NDVI')\n",
    "ax.plot(dates_str, ndre_means, 'ro', label='NDRE')\n",
    "ax.plot(dates_str, savi_means, 'yo', label='SAVI')\n",
    "ax.plot(dates_str, ndii_means, 'bo', label='NDII')\n",
    "ax.plot(dates_str, evi2_means, 'mo', label='EVI2')\n",
    "ax.plot(dates_str, ndvi_means, 'g')\n",
    "ax.plot(dates_str, ndre_means, 'r')\n",
    "ax.plot(dates_str, savi_means, 'y')\n",
    "ax.plot(dates_str, ndii_means, 'b')\n",
    "ax.plot(dates_str, evi2_means, 'm')\n",
    "plot_title = f\"Mean indices values from {start} to {end} of {getFileName(shapefile_path)}\"\n",
    "ax.set_title(plot_title)\n",
    "ax.set_ylabel('Index value')\n",
    "\n",
    "# checking if there is a need for a logarythmic scale\n",
    "if max(evi2_means) > 2:\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.savefig(filename_to_export + '.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating time series for further analysis\n",
    "Because of the cloud coverage and other factors, not all images can be used. Due to that fact, we do not have a regular timestep.\n",
    "In order to make time series, lacking values must be interpolated.\n",
    "\n",
    "The result is saved to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_tmp = pd.to_datetime(dates)\n",
    "df = pd.DataFrame({'Date': dates_tmp, 'NDVI': ndvi_means, 'NDRE': ndre_means, 'SAVI': savi_means, 'NDII': ndii_means, 'EVI2': evi2_means})\n",
    "date_steps = [(dates_str[i] - dates_str[i - 1]).days for i in range(1, len(dates_str))]\n",
    "min_timestep = min(date_steps)\n",
    "new_dates_df = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq=f'{min_timestep}D')\n",
    "new_ndvi_means_df = np.interp(\n",
    "    x=new_dates_df.astype(np.int64) / 10**9,  # new dates in int format\n",
    "    xp=df['Date'].astype(np.int64) / 10**9,   # original dates in int format\n",
    "    fp=df['NDVI']                             # original index values\n",
    ")\n",
    "new_ndre_means_df = np.interp(\n",
    "    x=new_dates_df.astype(np.int64) / 10**9,  \n",
    "    xp=df['Date'].astype(np.int64) / 10**9,   \n",
    "    fp=df['NDRE']                             \n",
    ")\n",
    "new_savi_means_df = np.interp(\n",
    "    x=new_dates_df.astype(np.int64) / 10**9,  \n",
    "    xp=df['Date'].astype(np.int64) / 10**9,   \n",
    "    fp=df['SAVI']                             \n",
    ")\n",
    "new_ndii_means_df = np.interp(\n",
    "    x=new_dates_df.astype(np.int64) / 10**9,  \n",
    "    xp=df['Date'].astype(np.int64) / 10**9,   \n",
    "    fp=df['NDII']                             \n",
    ")\n",
    "new_evi2_means_df = np.interp(\n",
    "    x=new_dates_df.astype(np.int64) / 10**9,  \n",
    "    xp=df['Date'].astype(np.int64) / 10**9,   \n",
    "    fp=df['EVI2'] \n",
    ")\n",
    "interpolated_df = pd.DataFrame({'Date': new_dates_df, 'NDVI': new_ndvi_means_df, 'NDRE': new_ndre_means_df, 'SAVI': new_savi_means_df, 'NDII': new_ndii_means_df, 'EVI2': new_evi2_means_df})\n",
    "\n",
    "interpolated_df.to_csv(filename_to_export + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map visualization of the last image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Plots values dated to {links_df['date'][len(links_df)-1]}\")\n",
    "\n",
    "#plt.figure(figsize=(3,3))\n",
    "\n",
    "# plt.imshow(scl_canal[0], cmap='gray')\n",
    "# plt.colorbar(label='Pixel value')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(red_canal[0], cmap='gray')\n",
    "# plt.colorbar(label='RED value')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(nir_canal[0], cmap='gray')\n",
    "# plt.colorbar(label='NIR value')\n",
    "# plt.show()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(3,3,1)\n",
    "plt.imshow(ndvi, cmap=\"RdYlGn\")\n",
    "plt.colorbar()\n",
    "plt.title('NDVI')\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(3,3,2)\n",
    "plt.imshow(ndre, cmap=\"RdYlGn\")\n",
    "plt.colorbar()\n",
    "plt.title('NDRE')\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(3,3,3)\n",
    "plt.imshow(savi, cmap=\"RdYlGn\")\n",
    "plt.colorbar()\n",
    "plt.title('SAVI')\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(3,3,4)\n",
    "plt.imshow(ndii, cmap=\"RdYlGn\")\n",
    "plt.colorbar()\n",
    "plt.title('NDII')\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(3,3,5)\n",
    "plt.imshow(evi2, cmap=\"RdYlGn\")\n",
    "plt.colorbar()\n",
    "plt.title('EVI2')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(nan_mask, cmap=\"gray\")\n",
    "# plt.colorbar(label='NaN Mask')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
